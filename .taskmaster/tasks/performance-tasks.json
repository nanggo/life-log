{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "포스트 메타데이터 압축 및 최적화",
        "description": "블로그 포스트 메타데이터의 크기를 줄이고 불필요한 데이터를 제거하여 초기 로딩 성능을 개선합니다.",
        "details": "현재 포스트 메타데이터에서 불필요한 필드 제거, 문자열 압축, 날짜 형식 최적화를 구현합니다. Svelte stores를 활용하여 메타데이터 캐싱을 구현하고, JSON.stringify 대신 구조화된 데이터 전달 방식을 도입합니다. 압축률 20% 달성을 목표로 하며, gzip/brotli 압축과 함께 적용하여 네트워크 전송량을 최소화합니다.",
        "testStrategy": "메타데이터 크기 측정 도구 작성, 압축 전후 번들 사이즈 비교, 네트워크 탭에서 전송량 확인, lighthouse 성능 점수 비교 측정",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "이미지 썸네일 최적화 및 LazyImage 성능 개선",
        "description": "GitHub API 썸네일 크기 최적화와 LazyImage 컴포넌트의 로딩 성능을 향상시킵니다.",
        "details": "GitHub API의 thumbnail 파라미터를 활용하여 적절한 크기의 이미지를 요청하도록 최적화합니다. Intersection Observer API를 사용한 LazyImage 컴포넌트 개선, 이미지 preloading 로직 추가, webp 포맷 지원 검토를 진행합니다. 이미지 로딩 상태 표시 및 에러 핸들링을 강화하여 사용자 경험을 개선합니다.",
        "testStrategy": "이미지 로딩 시간 측정, 네트워크 사용량 모니터링, 다양한 이미지 크기 테스트, 모바일 환경에서 로딩 속도 확인",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Virtual Scrolling 구현",
        "description": "20개 이상의 포스트가 있을 때 Virtual Scrolling을 도입하여 렌더링 성능을 향상시킵니다.",
        "details": "svelte-virtual-list 라이브러리 또는 직접 구현을 통해 Virtual Scrolling을 도입합니다. 포스트 개수에 따른 조건부 렌더링 로직 구현, DOM 요소 수 제한 (최대 20-30개), 스크롤 위치 유지 및 복원 기능을 포함합니다. SEO를 위해 SSG 시에는 전체 포스트를 렌더링하고, 클라이언트 하이드레이션 후에만 Virtual Scrolling을 활성화합니다.",
        "testStrategy": "대량 포스트 데이터로 스크롤 성능 테스트, 메모리 사용량 모니터링, SEO 크롤링 시뮬레이션, 다양한 화면 크기에서 테스트",
        "priority": "high",
        "dependencies": [1],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Vite 번들 분할 최적화",
        "description": "Vite 설정을 최적화하여 번들 크기를 줄이고 로딩 성능을 개선합니다.",
        "details": "vite.config.js에서 build.rollupOptions.output.manualChunks 설정을 통해 vendor 라이브러리, 컴포넌트, 유틸리티를 별도 청크로 분리합니다. dynamic import를 활용한 코드 스플리팅, tree-shaking 최적화, 불필요한 의존성 제거를 진행합니다. @rollup/plugin-analyzer를 사용하여 번들 분석을 수행하고 최적화 포인트를 식별합니다.",
        "testStrategy": "번들 분석기로 청크 크기 확인, 로딩 시간 측정, lighthouse 성능 점수 비교, 캐시 효율성 검증",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "HTML 헤딩 추출 최적화",
        "description": "블로그 포스트의 목차 생성을 위한 HTML 헤딩 추출 과정을 최적화합니다.",
        "details": "MDSvex 컴파일 시점에 헤딩 정보를 추출하여 메타데이터에 포함시키는 방식으로 런타임 파싱을 제거합니다. rehype-extract-headings 플러그인 활용 또는 커스텀 플러그인 개발을 통해 빌드 타임에 목차 데이터를 생성합니다. 생성된 헤딩 데이터는 포스트 메타데이터와 함께 번들에 포함되어 즉시 사용 가능하도록 합니다.",
        "testStrategy": "헤딩 추출 성능 측정, 빌드 시간 비교, 생성된 목차 데이터 정확성 검증, 다양한 마크다운 구조 테스트",
        "priority": "medium",
        "dependencies": [1],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "브라우저 캐싱 전략 개선",
        "description": "정적 자산과 API 응답에 대한 브라우저 캐싱을 최적화합니다.",
        "details": "Vercel 또는 배포 플랫폼의 헤더 설정을 통해 정적 자산(JS, CSS, 이미지)에 대한 긴 캐시 기간 설정, 해시 기반 파일명을 통한 캐시 무효화 전략을 구현합니다. Service Worker 기반 캐싱 전략 도입을 검토하고, workbox-build를 활용한 캐시 관리 시스템을 구축합니다. JSON 데이터에 대한 ETags 지원도 고려합니다.",
        "testStrategy": "캐시 헤더 설정 확인, 캐시 적중률 측정, 업데이트 후 캐시 무효화 테스트, 다양한 브라우저에서 캐싱 동작 검증",
        "priority": "medium",
        "dependencies": [4],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "태그 필터링 성능 최적화",
        "description": "태그 기반 포스트 필터링의 성능을 개선하고 응답성을 향상시킵니다.",
        "details": "태그별 포스트 인덱스를 빌드 타임에 생성하여 런타임 필터링 성능을 개선합니다. Map 또는 Set 자료구조를 활용한 O(1) 태그 검색, debounce를 적용한 필터링 입력 처리, Virtual Scrolling과 연동된 효율적인 필터 결과 렌더링을 구현합니다. 인기 태그에 대한 사전 계산된 결과 캐싱도 고려합니다.",
        "testStrategy": "대량 포스트와 태그에서 필터링 속도 측정, 메모리 사용량 모니터링, 사용자 인터랙션 반응성 테스트, 다양한 필터 조합 성능 검증",
        "priority": "medium",
        "dependencies": [3, 5],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "성능 모니터링 및 최적화 검증",
        "description": "구현된 최적화의 효과를 측정하고 성능 목표 달성을 검증합니다.",
        "details": "Lighthouse CI를 통한 자동화된 성능 측정, Web Vitals 모니터링 설정, 번들 사이즈 추적 도구 구성을 진행합니다. 최적화 전후 성능 비교 리포트 생성, Core Web Vitals (LCP, FID, CLS) 개선 확인, SEO 점수 유지 검증을 수행합니다. 성능 회귀 방지를 위한 CI/CD 파이프라인 통합도 고려합니다.",
        "testStrategy": "Lighthouse 점수 비교, Web Vitals 측정, 번들 사이즈 변화 추적, 실제 사용자 시뮬레이션 테스트, SEO 크롤링 검증",
        "priority": "high",
        "dependencies": [2, 3, 4, 6, 7],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-03T16:18:39.022Z",
      "updated": "2025-08-03T16:18:56.940Z",
      "description": "Tasks for master context"
    }
  }
}
